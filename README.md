# DSC253_group_project
 
## Introduction
Low-resource languages are characterized by a scarcity of resources, limited study, and a smaller presence in language technology applications. The study of LRLs using Natural Language Processing (NLP) tools is crucial for the preservation of these languages, promoting inclusivity and enhancing our understanding of underrepresented human cultures.

## Models Evaluated
**M2M-100**  
Developed by Facebook AI, M2M-100 translates directly between any pair of 100 languages without relying on English as an intermediary.

**NLLB-200**  
No Language Left Behind (NLLB) by Meta supports over 200 languages, using advanced techniques like bitext mining and curriculum training to improve LRL translation quality.

**mBART50**  
A multilingual sequence-to-sequence denoising auto-encoder, trained on 50 languages, with both many-to-many and many-to-one configurations.

**SMaLL-100**  
A compact and fast massively multilingual MT model covering more than 10K language pairs, achieving competitive results with M2M-100 while being much smaller and faster.

## Contributors
* **Jiaxian Xiang** (j1xiang@ucsd.edu)  
* **Shanglin Zeng** (s6zeng@ucsd.edu)  
* **Ariane Yu** (yiy033@ucsd.edu)  
* **Lucas Lee** (l2lee@ucsd.edu)  
